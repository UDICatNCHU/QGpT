# -*- coding: utf-8 -*-
"""
QGpT: Improving Table Retrieval with Question Generation from Partial Tables
Query Evaluator with Multi-K Recall Metrics

This script evaluates query performance using test queries against built vector databases.
Now supports Recall@1, Recall@3, Recall@5, Recall@10 evaluation.

Usage:
    python query_evaluator.py "query text" --db database.db
    python query_evaluator.py --test-file test_queries.json --db database.db
    python query_evaluator.py --batch-eval  # æ‰¹æ¬¡è©•ä¼°æ‰€æœ‰æ¸¬è©¦é›†

Author: QGpT Research Team
Repository: https://github.com/UDICatNCHU/QGpT
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from pymilvus import MilvusClient
from FlagEmbedding import BGEM3FlagModel

from utils import (
    load_json_dataset,
    format_search_results,
    get_corpus_files,
    extract_corpus_name_from_path,
    generate_db_name,
    generate_collection_name
)
from embedding_models import EmbeddingModel, MODELS

# å›ºå®šçš„è©•ä¼° K å€¼
EVAL_K_VALUES = [1, 3, 5, 10]


def normalize_filename(filepath: str) -> str:
    """æ¨™æº–åŒ–æª”æ¡ˆè·¯å¾‘ï¼Œåªä¿ç•™æª”å"""
    if isinstance(filepath, str):
        return filepath.split('/')[-1].strip()
    return str(filepath)


def calculate_recall_at_k(retrieved_files: List[str], ground_truth: List[str], 
                         k_values: List[int] = EVAL_K_VALUES) -> Dict[str, float]:
    """
    è¨ˆç®—å¤šå€‹ K å€¼çš„å¬å›ç‡æŒ‡æ¨™
    
    Args:
        retrieved_files: æª¢ç´¢åˆ°çš„æª”æ¡ˆåˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        ground_truth: æ­£ç¢ºç­”æ¡ˆæª”æ¡ˆåˆ—è¡¨
        k_values: è¦è¨ˆç®—çš„ K å€¼åˆ—è¡¨
        
    Returns:
        å„å€‹ K å€¼çš„å¬å›ç‡å­—å…¸
    """
    if not ground_truth:
        return {f'recall_at_{k}': 0.0 for k in k_values}
    
    normalized_ground_truth = [normalize_filename(gt) for gt in ground_truth]
    normalized_retrieved = [normalize_filename(rf) for rf in retrieved_files]
    
    recall_metrics = {}
    for k in k_values:
        # åªè€ƒæ…®å‰ k å€‹çµæœ
        top_k_retrieved = normalized_retrieved[:k]
        hits = sum(1 for gt in normalized_ground_truth if gt in top_k_retrieved)
        recall_metrics[f'recall_at_{k}'] = hits / len(ground_truth)
    
    return recall_metrics


class QGpTQueryEvaluator:
    """QGpT æŸ¥è©¢è©•ä¼°å™¨"""
    
    def __init__(self, db_path: str, collection_name: str, model: str = "bge_m3_flag"):
        """
        åˆå§‹åŒ–æŸ¥è©¢è©•ä¼°å™¨
        
        Args:
            db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘
            collection_name: å‘é‡é›†åˆåç¨±
        """
        self.db_path = db_path
        self.collection_name = collection_name
        self.client = None
        self.embedding_fn = None
        self.initialize(model)
    
    def initialize(self, model):
        """åˆå§‹åŒ– Milvus å®¢æˆ¶ç«¯å’ŒåµŒå…¥å‡½æ•¸"""
        try:
            if not Path(self.db_path).exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è³‡æ–™åº«æª”æ¡ˆ: {self.db_path}")
            
            self.client = MilvusClient(self.db_path)
            print("ğŸ”„ åˆå§‹åŒ– BGE-M3 æ¨¡å‹...")
            self.embedding_fn = MODELS.get(model)()

            print("âœ… BGE-M3 æ¨¡å‹è¼‰å…¥å®Œæˆ")
            
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.client.has_collection(collection_name=self.collection_name):
                raise ValueError(f"æ‰¾ä¸åˆ°é›†åˆ '{self.collection_name}' åœ¨è³‡æ–™åº« '{self.db_path}'")
            
            print(f"âœ… æˆåŠŸé€£æ¥åˆ°è³‡æ–™åº«: {self.db_path}")
            print(f"âœ… ä½¿ç”¨é›†åˆ: {self.collection_name}")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
            sys.exit(1)
    
    def search(self, query: str, limit: int = 10) -> List[Dict]:
        """
        åŸ·è¡Œæœç´¢æŸ¥è©¢
        
        Args:
            query: æœç´¢æŸ¥è©¢å­—ç¬¦ä¸²
            limit: è¿”å›çµæœçš„æ•¸é‡
            
        Returns:
            æœç´¢çµæœåˆ—è¡¨
        """
        try:
            # å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ (ä½¿ç”¨ BGE-M3)
            query_vector = self.embedding_fn.encode([query])['dense_vecs'][0].astype('float32').tolist()
            
            # åŸ·è¡Œæœç´¢
            search_results = self.client.search(
                collection_name=self.collection_name,
                data=[query_vector],  # éœ€è¦åŒ…è£æˆåˆ—è¡¨
                limit=limit,
                output_fields=["text", "filename", "sheet_name", "original_id"]
            )
            
            # æ ¼å¼åŒ–çµæœ
            formatted_results = []
            for result in search_results[0]:
                formatted_results.append({
                    'score': 1 - result['distance'],  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                    'distance': result['distance'],
                    'filename': result['entity']['filename'],
                    'sheet_name': result['entity']['sheet_name'],
                    'original_id': result['entity']['original_id'],
                    'text': result['entity']['text']
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ æœç´¢éŒ¯èª¤: {e}")
            return []
    
    def evaluate_single_query(self, query: str, ground_truth: Optional[List[str]] = None) -> Dict:
        """
        è©•ä¼°å–®ä¸€æŸ¥è©¢ï¼Œè¨ˆç®—å¤šå€‹ K å€¼çš„å¬å›ç‡
        
        Args:
            query: æŸ¥è©¢å­—ç¬¦ä¸²
            ground_truth: æ­£ç¢ºç­”æ¡ˆåˆ—è¡¨ï¼ˆæª”æ¡ˆåæˆ–IDï¼‰
            
        Returns:
            è©•ä¼°çµæœï¼ŒåŒ…å«å¤šå€‹ K å€¼çš„å¬å›ç‡
        """
        # æœå°‹ top-10 çµæœï¼ˆè¶³å¤ è¨ˆç®—æ‰€æœ‰ K å€¼ï¼‰
        results = self.search(query, max(EVAL_K_VALUES))
        
        evaluation = {
            'query': query,
            'results_count': len(results),
            'results': results
        }
        
        # å¦‚æœæœ‰æ­£ç¢ºç­”æ¡ˆï¼Œè¨ˆç®—å¬å›ç‡æŒ‡æ¨™
        if ground_truth:
            retrieved_files = [r['filename'] for r in results]
            recall_metrics = calculate_recall_at_k(retrieved_files, ground_truth, EVAL_K_VALUES)
            evaluation.update(recall_metrics)
        
        return evaluation


class BatchEvaluator:
    """æ‰¹æ¬¡è©•ä¼°å™¨"""
    
    def __init__(self):
        self.test_files_dir = "Test_Query_and_GroundTruth_Table"
    
    def get_test_files(self) -> List[str]:
        """å–å¾—æ‰€æœ‰æ¸¬è©¦æª”æ¡ˆ"""
        test_dir = Path(self.test_files_dir)
        if not test_dir.exists():
            return []
        
        return [str(f) for f in test_dir.glob("*.json")]
    
    def match_corpus_to_test(self, test_file: str) -> Optional[str]:
        """
        å°‡æ¸¬è©¦æª”æ¡ˆåŒ¹é…åˆ°å°æ‡‰çš„èªæ–™åº«
        
        Args:
            test_file: æ¸¬è©¦æª”æ¡ˆè·¯å¾‘
            
        Returns:
            å°æ‡‰çš„èªæ–™åº«è³‡æ–™åº«è·¯å¾‘ï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å› None
        """
        test_name = Path(test_file).stem
        
        # å®šç¾©æ¸¬è©¦æª”æ¡ˆåˆ°èªæ–™åº«çš„æ˜ å°„è¦å‰‡
        mapping_rules = {
            'MiMoTable-English': 'Table1_mimo_table_length_variation_mimo_en',
            'MiMoTable-Chinese': 'Table1_mimo_table_length_variation_mimo_ch',
            'E2E-WTQ': 'Table5_Single_Table_Retrieval_QGpT',
            'FetaQA': 'Table5_Single_Table_Retrieval_QGpT',
            'OTT-QA': 'Table7_OTTQA',
            'MMQA-2tables': 'Table6_Multi_Table_Retrieval_2_tables',
            'MMQA-3tables': 'Table6_Multi_Table_Retrieval_3_tables'
        }
        
        # å°‹æ‰¾åŒ¹é…çš„èªæ–™åº«
        for test_key, corpus_pattern in mapping_rules.items():
            if test_key in test_name:
                # å°‹æ‰¾å°æ‡‰çš„è³‡æ–™åº«æª”æ¡ˆ
                corpus_files = get_corpus_files()
                for corpus_info in corpus_files:
                    if corpus_pattern in corpus_info['name']:
                        db_path = corpus_info['db_name']
                        if Path(db_path).exists():
                            return db_path
        
        return None
    
    def evaluate_test_file(self, test_file: str, db_path: str, model: str = "bge_m3_flag") -> Dict:
        """
        è©•ä¼°æ¸¬è©¦æª”æ¡ˆï¼Œè¨ˆç®—å¤šå€‹ K å€¼çš„å¹³å‡å¬å›ç‡
        
        Args:
            test_file: æ¸¬è©¦æª”æ¡ˆè·¯å¾‘
            db_path: è³‡æ–™åº«è·¯å¾‘
            
        Returns:
            è©•ä¼°çµæœï¼ŒåŒ…å«å„å€‹ K å€¼çš„å¹³å‡å¬å›ç‡
        """
        # è¼‰å…¥æ¸¬è©¦è³‡æ–™
        test_data = load_json_dataset(test_file)
        
        # æ ¹æ“šè³‡æ–™åº«è·¯å¾‘ç”Ÿæˆé›†åˆåç¨±
        corpus_name = Path(db_path).stem.replace('qgpt_', '')
        collection_name = generate_collection_name(corpus_name)
        
        # åˆå§‹åŒ–è©•ä¼°å™¨
        evaluator = QGpTQueryEvaluator(db_path, collection_name, model)
        
        results = []
        recall_sums = {f'recall_at_{k}': 0.0 for k in EVAL_K_VALUES}
        
        print(f"ğŸ”„ è©•ä¼°æ¸¬è©¦æª”æ¡ˆ: {Path(test_file).name}")
        print(f"   æŸ¥è©¢æ•¸é‡: {len(test_data)}")
        
        for i, item in enumerate(test_data):
            query = item.get('question', '')
            
            # æå–æ­£ç¢ºç­”æ¡ˆï¼ˆæ ¹æ“šæ¸¬è©¦æª”æ¡ˆçµæ§‹èª¿æ•´ï¼‰
            ground_truth = []
            if 'spreadsheet_list' in item:
                ground_truth = item['spreadsheet_list']
            elif 'answer' in item:
                # æŸäº›æ¸¬è©¦æª”æ¡ˆå¯èƒ½æœ‰ä¸åŒçš„çµæ§‹
                pass
            
            # åŸ·è¡Œè©•ä¼°
            eval_result = evaluator.evaluate_single_query(query, ground_truth)
            results.append(eval_result)
            
            # ç´¯è¨ˆå„å€‹ K å€¼çš„å¬å›ç‡
            for k in EVAL_K_VALUES:
                recall_key = f'recall_at_{k}'
                if recall_key in eval_result:
                    recall_sums[recall_key] += eval_result[recall_key]
            
            # é¡¯ç¤ºé€²åº¦
            if (i + 1) % 10 == 0:
                print(f"   è™•ç†é€²åº¦: {i + 1}/{len(test_data)}")
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_recalls = {}
        for k in EVAL_K_VALUES:
            recall_key = f'recall_at_{k}'
            avg_recalls[f'avg_{recall_key}'] = recall_sums[recall_key] / len(test_data) if test_data else 0.0
        
        # å„²å­˜å€‹åˆ¥æ¸¬è©¦æª”æ¡ˆçµæœ
        experiment_dir = Path("experiment")
        experiment_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå€‹åˆ¥çµæœæª”æ¡ˆåç¨±ï¼šmodel_name_db_name.json
        db_name = Path(db_path).stem.replace('qgpt_', '')
        individual_result_file = experiment_dir / f"{model}_{db_name}.json"
        
        result_data = {
            'test_file': test_file,
            'db_path': db_path,
            'model': model,
            'total_queries': len(test_data),
            **avg_recalls,
            'detailed_results': results
        }
        
        with open(individual_result_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ å€‹åˆ¥çµæœå·²å„²å­˜åˆ°: {individual_result_file}")
        
        return result_data
    
    def run_batch_evaluation(self, save_results: bool = True, model: str = "bge_m3_flag") -> Dict:
        """
        åŸ·è¡Œæ‰¹æ¬¡è©•ä¼°ï¼Œè¨ˆç®—æ‰€æœ‰æ¸¬è©¦é›†çš„å¤š K å€¼å¬å›ç‡
        
        Args:
            save_results: æ˜¯å¦å„²å­˜è©³ç´°çµæœ
            
        Returns:
            æ‰¹æ¬¡è©•ä¼°çµæœ
        """
        test_files = self.get_test_files()
        if not test_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æ¸¬è©¦æª”æ¡ˆ")
            return {}
        
        batch_results = {}
        
        print(f"ğŸ”„ é–‹å§‹æ‰¹æ¬¡è©•ä¼°ï¼Œæ‰¾åˆ° {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆ")
        print(f"ğŸ“Š è©•ä¼°æŒ‡æ¨™: {', '.join(f'Recall@{k}' for k in EVAL_K_VALUES)}")
        
        for test_file in test_files:
            print(f"\n{'='*60}")
            
            # å°‹æ‰¾å°æ‡‰çš„èªæ–™åº«è³‡æ–™åº«
            db_path = self.match_corpus_to_test(test_file)
            if not db_path:
                print(f"âš ï¸  è·³é {Path(test_file).name}ï¼šæ‰¾ä¸åˆ°å°æ‡‰çš„èªæ–™åº«è³‡æ–™åº«")
                continue
            
            try:
                # åŸ·è¡Œè©•ä¼°
                result = self.evaluate_test_file(test_file, db_path, model)
                batch_results[Path(test_file).stem] = result
                
                print(f"âœ… å®Œæˆè©•ä¼°: {Path(test_file).name}")
                for k in EVAL_K_VALUES:
                    recall_value = result[f'avg_recall_at_{k}']
                    print(f"   Recall@{k}: {recall_value:.4f}")
                
            except Exception as e:
                print(f"âŒ è©•ä¼°å¤±æ•—: {Path(test_file).name} - {e}")
        
        # å„²å­˜çµæœ
        if save_results and batch_results:
            # å‰µå»º experiment ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            experiment_dir = Path("experiment")
            experiment_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆçµæœæª”æ¡ˆåç¨±ï¼šmodel_name_overall_results.json
            results_file = experiment_dir / f"{model}_overall_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(batch_results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ è©•ä¼°çµæœå·²å„²å­˜åˆ°: {results_file}")
        
        return batch_results


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description='QGpT æŸ¥è©¢è©•ä¼°å™¨ - å¤š K å€¼å¬å›ç‡ç‰ˆæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # å–®ä¸€æŸ¥è©¢æ¸¬è©¦ï¼ˆæœƒè¨ˆç®— Recall@1,3,5,10ï¼‰
  python query_evaluator.py "è²¡å‹™å ±è¡¨" --db qgpt_Table1_mimo_ch.db
  
  # ä½¿ç”¨æ¸¬è©¦æª”æ¡ˆè©•ä¼°
  python query_evaluator.py --test-file Test_Query_and_GroundTruth_Table/MiMoTable-English_test.json --db qgpt_Table1_mimo_en.db
  
  # æ‰¹æ¬¡è©•ä¼°æ‰€æœ‰æ¸¬è©¦é›†ï¼ˆè¨ˆç®—å¤šå€‹ K å€¼çš„å¬å›ç‡ï¼‰
  python query_evaluator.py --batch-eval
        """
    )
    
    parser.add_argument('query', nargs='?', help='æœç´¢æŸ¥è©¢å­—ç¬¦ä¸²')
    parser.add_argument('--db', help='å‘é‡è³‡æ–™åº«è·¯å¾‘')
    parser.add_argument('--collection', help='å‘é‡é›†åˆåç¨±ï¼ˆè‡ªå‹•å¾è³‡æ–™åº«åç¨±æ¨å°ï¼‰')
    parser.add_argument('--test-file', help='æ¸¬è©¦æŸ¥è©¢æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--batch-eval', action='store_true', help='æ‰¹æ¬¡è©•ä¼°æ‰€æœ‰æ¸¬è©¦é›†')
    parser.add_argument('--limit', type=int, default=10, help='é¡¯ç¤ºçµæœæ•¸é‡ (é è¨­: 10)')
    parser.add_argument('--format', choices=['detailed', 'simple', 'json'], 
                       default='detailed', help='è¼¸å‡ºæ ¼å¼ (é è¨­: detailed)')
    parser.add_argument('--save', action='store_true', help='å„²å­˜è©•ä¼°çµæœåˆ°æª”æ¡ˆ')
    parser.add_argument('--model', default="bge_m3_flag", help='jina_colbert_v2 or bge_m3_flag (é è¨­: bge_m3_flag)')

    args = parser.parse_args()
    
    # æ‰¹æ¬¡è©•ä¼°
    if args.batch_eval:
        evaluator = BatchEvaluator()
        results = evaluator.run_batch_evaluation(save_results=args.save, model=args.model)
        
        # é¡¯ç¤ºç¸½çµ
        if results:
            print(f"\n{'='*60}")
            print("æ‰¹æ¬¡è©•ä¼°ç¸½çµ:")
            for test_name, result in results.items():
                print(f"  {test_name}:")
                print(f"    æŸ¥è©¢æ•¸é‡: {result['total_queries']}")
                for k in EVAL_K_VALUES:
                    recall_value = result[f'avg_recall_at_{k}']
                    print(f"    Recall@{k}: {recall_value:.4f}")
        
        return
    
    # å–®ä¸€æŸ¥è©¢æˆ–æ¸¬è©¦æª”æ¡ˆè©•ä¼°
    if not args.db:
        print("âŒ è«‹æä¾›è³‡æ–™åº«è·¯å¾‘ (--db)")
        sys.exit(1)
    
    # è‡ªå‹•ç”Ÿæˆé›†åˆåç¨±
    if not args.collection:
        corpus_name = Path(args.db).stem.replace('qgpt_', '')
        collection_name = generate_collection_name(corpus_name)
    else:
        collection_name = args.collection
    
    # åˆå§‹åŒ–è©•ä¼°å™¨
    evaluator = QGpTQueryEvaluator(args.db, collection_name, args.model)
    
    # æ¸¬è©¦æª”æ¡ˆè©•ä¼°
    if args.test_file:
        if not Path(args.test_file).exists():
            print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦æª”æ¡ˆ: {args.test_file}")
            sys.exit(1)
        
        batch_evaluator = BatchEvaluator()
        result = batch_evaluator.evaluate_test_file(args.test_file, args.db, args.model)
        
        print(f"\nè©•ä¼°çµæœ:")
        print(f"æ¸¬è©¦æª”æ¡ˆ: {result['test_file']}")
        print(f"æŸ¥è©¢ç¸½æ•¸: {result['total_queries']}")
        for k in EVAL_K_VALUES:
            recall_value = result[f'avg_recall_at_{k}']
            print(f"Recall@{k}: {recall_value:.4f}")
        
        if args.save:
            # å‰µå»º experiment ç›®éŒ„
            experiment_dir = Path("experiment")
            experiment_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆçµæœæª”æ¡ˆåç¨±ï¼šmodel_name_db_name.json
            db_name = Path(args.db).stem.replace('qgpt_', '')
            results_file = experiment_dir / f"{args.model}_{db_name}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"è©³ç´°çµæœå·²å„²å­˜åˆ°: {results_file}")
        
        return
    
    # å–®ä¸€æŸ¥è©¢
    if args.query:
        results = evaluator.search(args.query, args.limit)
        output = format_search_results(results, args.query, args.format)
        print(output)
        return
    
    # å¦‚æœæ²’æœ‰æä¾›åƒæ•¸ï¼Œé¡¯ç¤ºå¹«åŠ©
    parser.print_help()


if __name__ == "__main__":
    main()